## Методы сбора и обработки данных из сети Интернет.

### Урок 1. Основы клиент-серверного взаимодействия. Парсинг API.


**Задание:**

Организовать сбор данных с сайта https://5ka.ru/special_offers/:

* Необходимо реализовать метод сохранения данных в .json файлы.
* Данные скачиваются с источника, при вызове метода/функции сохранения в файл.
* Для каждой категории товаров должен быть создан отдельный файл и содержать товары исключительно соответсвующие данной категории.

Пример структуры данных для файла:

{

"name": "имя категории",

"code": "Код соответсвующий категории (используется в запросах)",

"products": [{PRODUCT}, {PRODUCT}........] # список словарей товаров соответсвующих данной категории

}


**Примечание:**

>_Нейминг ключей можно делать отличным от примера._


### Урок 2. Парсинг HTML. BeautifulSoup, MongoDB

#### Задание:

Необходимо обойти все записи в блоге:https://gb.ru/posts/ и извлеч из них информацию следующих полей:

* url страницы материала;
* заголовок материала;
* первое изображение материала (ссылка);
* дата публикации (в формате datetime);
* имя автора материала;
* ссылка на страницу автора материала;
* комментарии в виде (автор комментария и текст комментария).

Структуру сохранить в MongoDB.

### Урок 3. Системы управления базами данных MongoDB и SQLite в Python.

#### Задание:
* Продолжить работу с парсером блога GB;
* Реализовать SQL базу данных посредствам SQLAlchemy;
* Реализовать реалиционные связи между: Постом и Автором, Постом и Тегом, Постом и комментарием, Комментарием и комментарием;
